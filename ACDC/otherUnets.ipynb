{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"otherUnets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPjhpFcr0MYBsdXpMU1nte5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["MIN_MERGE = 4\n"," \n","def calcMinRun(n):\n","    \"\"\"Returns the minimum length of a\n","    run from 23 - 64 so that\n","    the len(array)/minrun is less than or\n","    equal to a power of 2.\n"," \n","    e.g. 1=>1, ..., 63=>63, 64=>32, 65=>33,\n","    ..., 127=>64, 128=>32, ...\n","    \"\"\"\n","    r = 0\n","    while n >= MIN_MERGE:\n","        r |= n & 1\n","        n >>= 1\n","    return n + r\n"," \n"," \n","# This function sorts array from left index to\n","# to right index which is of size atmost RUN\n","def insertionSort(arr, left, right):\n","    for i in range(left + 1, right + 1):\n","        j = i\n","        while j > left and arr[j] < arr[j - 1]:\n","            arr[j], arr[j - 1] = arr[j - 1], arr[j]\n","            j -= 1\n"," \n"," \n","# Merge function merges the sorted runs\n","def merge(arr, l, m, r):\n","     \n","    # original array is broken in two parts\n","    # left and right array\n","    left = arr[l:m+1]\n","    right = arr[m+1:r+1]\n","    # after comparing, we merge those two array\n","    # in larger sub array\n","    k = l\n","    while left and right:\n","        if left[0] <= right[0]:\n","            arr[k] = left.pop(0)\n","        else:\n","            arr[k] = right.pop(0)\n","        k += 1\n","    # Copy remaining elements of left, if any\n","    if left:\n","        arr[k:r+1] = left\n"," \n","    # Copy remaining element of right, if any\n","    if right:\n","        arr[k:r+1] = right\n"," \n"," \n","# Iterative Timsort function to sort the\n","# array[0...n-1] (similar to merge sort)\n","def timSort(arr):\n","    n = len(arr)\n","    minRun = calcMinRun(n)\n","     \n","    # Sort individual subarrays of size RUN\n","    for start in range(0, n, minRun):\n","        end = min(start + minRun - 1, n - 1)\n","        insertionSort(arr, start, end)\n","    # Start merging from size RUN (or 32). It will merge\n","    # to form size 64, then 128, 256 and so on ....\n","    size = minRun\n","    while size < n:\n","         \n","        # Pick starting point of left sub array. We\n","        # are going to merge arr[left..left+size-1]\n","        # and arr[left+size, left+2*size-1]\n","        # After every merge, we increase left by 2*size\n","        for left in range(0, n, 2 * size):\n"," \n","            # Find ending point of left sub array\n","            # mid+1 is starting point of right sub array\n","            mid = min(n - 1, left + size - 1)\n","            right = min((left + 2 * size - 1), (n - 1))\n"," \n","            # Merge sub array arr[left.....mid] &\n","            # arr[mid+1....right]\n","            if mid < right:\n","                merge(arr, left, mid, right)\n","                \n"," \n","        size = 2 * size"],"metadata":{"id":"MrP3j1O_KkI-","executionInfo":{"status":"ok","timestamp":1644601967769,"user_tz":-420,"elapsed":395,"user":{"displayName":"Minh Nhat Trinh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUlruUlJibAIqBqbqLK2Tos6lKVv00A2KZfuwF=s64","userId":"12425805762404293245"}}},"execution_count":161,"outputs":[]},{"cell_type":"code","source":["arr = [-2, 7, 15, -14, 0, 15, 0,\n","        7, -7, -4, -13, 5, 8, -14, 12]\n","\n","print(\"Given Array is\")\n","print(arr)\n","\n","# Function Call\n","timSort(arr)\n","\n","print(\"After Sorting Array is\")\n","print(arr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cQKLpm1D5NMb","executionInfo":{"status":"ok","timestamp":1644601969126,"user_tz":-420,"elapsed":425,"user":{"displayName":"Minh Nhat Trinh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUlruUlJibAIqBqbqLK2Tos6lKVv00A2KZfuwF=s64","userId":"12425805762404293245"}},"outputId":"8b4b2ee7-b7a9-421a-cab5-b03a0691729c"},"execution_count":162,"outputs":[{"output_type":"stream","name":"stdout","text":["Given Array is\n","[-2, 7, 15, -14, 0, 15, 0, 7, -7, -4, -13, 5, 8, -14, 12]\n","After Sorting Array is\n","[-14, -14, -13, -7, -4, -2, 0, 0, 5, 7, 7, 8, 12, 15, 15]\n"]}]},{"cell_type":"code","metadata":{"id":"oUdMCotlpSWX"},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","from torchsummary import summary\n","from torch.nn import init"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBSNO_MRpdF_"},"source":["# https://github.com/miguelvr/dropblock/blob/master/dropblock/dropblock.py\n","class DropBlock2D(nn.Module):\n","    def __init__(self, drop_prob, block_size):\n","        super(DropBlock2D, self).__init__()\n","        self.drop_prob = drop_prob\n","        self.block_size = block_size\n","    def forward(self, x):\n","        # shape: (bsize, channels, height, width)\n","        assert x.dim() == 4, \\\n","            \"Expected input with 4 dimensions (bsize, channels, height, width)\"\n","        if not self.training or self.drop_prob == 0.:\n","            return x\n","        else:\n","            # get gamma value\n","            gamma = self.drop_prob / (self.block_size ** 2)\n","            # sample mask\n","            mask = (torch.rand(x.shape[0], *x.shape[2:], device= x.device) < gamma).float()\n","            # compute block mask\n","            block_mask = self._compute_block_mask(mask)\n","            # apply block mask\n","            out = x * block_mask[:, None, :, :]\n","            # scale output\n","            out = out * block_mask.numel() / block_mask.sum()\n","            return out\n","    def _compute_block_mask(self, mask):\n","        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n","                                  kernel_size=(self.block_size, self.block_size),\n","                                  stride=(1, 1),\n","                                  padding=self.block_size // 2)\n","\n","        if self.block_size % 2 == 0:\n","            block_mask = block_mask[:, :, :-1, :-1]\n","        block_mask = 1 - block_mask.squeeze(1)\n","        return block_mask\n","\n","class DropBlock3D(DropBlock2D):\n","    def __init__(self, drop_prob, block_size):\n","        super(DropBlock3D, self).__init__(drop_prob, block_size)\n","    def forward(self, x):\n","        # shape: (bsize, channels, depth, height, width)\n","        assert x.dim() == 5, \\\n","            \"Expected input with 5 dimensions (bsize, channels, depth, height, width)\"\n","        if not self.training or self.drop_prob == 0.:\n","            return x\n","        else:\n","            # get gamma value\n","            gamma = self.drop_prob / (self.block_size ** 3)\n","            # sample mask\n","            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n","            # place mask on input device\n","            mask = mask.to(x.device)\n","            # compute block mask\n","            block_mask = self._compute_block_mask(mask)\n","            # apply block mask\n","            out = x * block_mask[:, None, :, :, :]\n","            # scale output\n","            out = out * block_mask.numel() / block_mask.sum()\n","            return out\n","    def _compute_block_mask(self, mask):\n","        block_mask = F.max_pool3d(input=mask[:, None, :, :, :],\n","                                  kernel_size=(self.block_size, self.block_size, self.block_size),\n","                                  stride=(1, 1, 1),\n","                                  padding=self.block_size // 2)\n","        if self.block_size % 2 == 0:\n","            block_mask = block_mask[:, :, :-1, :-1, :-1]\n","        block_mask = 1 - block_mask.squeeze(1)\n","        return block_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2X14iVipeYy"},"source":["class conv_block(nn.Sequential):\n","    def __init__(self, ch_in, ch_out, kernel_size = 3, \n","                 padding = 1, drop_block=False, block_size = 1, drop_prob = 0):\n","        super().__init__()\n","        self.add_module(\"conv1\",nn.Conv2d(ch_in, ch_out, kernel_size, padding = padding,bias=False))\n","        self.add_module(\"bn1\", nn.BatchNorm2d(ch_out))\n","        self.add_module(\"relu1\", nn.ReLU(inplace=True))\n","        self.add_module(\"conv2\",nn.Conv2d(ch_out, ch_out, kernel_size, padding = padding,bias=False))\n","        if drop_block:\n","            self.add_module(\"drop_block\", DropBlock2D(block_size = block_size, drop_prob = drop_prob))\n","        self.add_module(\"bn2\", nn.BatchNorm2d(ch_out))\n","        self.add_module(\"relu2\", nn.ReLU(inplace=True))\n","\n","class up_conv(nn.Module):\n","    def __init__(self,ch_in,ch_out):\n","        super(up_conv,self).__init__()\n","        self.up = nn.Sequential(\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(ch_in,ch_out,kernel_size=2,stride=1,padding=\"same\",bias=False,),\n","\t\t    nn.BatchNorm2d(ch_out),\n","\t\t\tnn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self,x):\n","        x = self.up(x)\n","        return x\n","\n","class U_Net(nn.Module):\n","    def __init__(self,img_ch=1,output_ch=4, drop_prob = 0):\n","        super(U_Net,self).__init__()\n","        \n","        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n","        channel = 64\n","        self.Conv1 = conv_block(ch_in=img_ch,ch_out=channel)\n","        self.Conv2 = conv_block(ch_in=channel,ch_out=channel*2)\n","        self.Conv3 = conv_block(ch_in=channel*2,ch_out=channel*4)\n","        self.Conv4 = conv_block(ch_in=channel*4,ch_out=channel*8, drop_block=True, block_size = 5, drop_prob = drop_prob)\n","        self.Conv5 = conv_block(ch_in=channel*8,ch_out=channel*16, drop_block=True, block_size = 3, drop_prob = drop_prob)\n","\n","        self.Up5 = up_conv(ch_in=channel*16,ch_out=channel*8)\n","        self.Up_conv5 = conv_block(ch_in=channel*16, ch_out=channel*8)\n","\n","        self.Up4 = up_conv(ch_in=channel*8,ch_out=channel*4)\n","        self.Up_conv4 = conv_block(ch_in=channel*8, ch_out=channel*4)\n","        \n","        self.Up3 = up_conv(ch_in=channel*4,ch_out=channel*2)\n","        self.Up_conv3 = conv_block(ch_in=channel*4, ch_out=channel*2)\n","        \n","        self.Up2 = up_conv(ch_in=channel*2,ch_out=channel)\n","        self.Up_conv2 = conv_block(ch_in=channel*2, ch_out=channel)\n","\n","        self.Conv_1x1 = nn.Sequential(\n","            nn.Conv2d(channel, output_ch,kernel_size=1,stride=1,padding=0), \n","            nn.Softmax(dim=1)\n","            )\n","\n","\n","    def forward(self,x):\n","        # encoding path\n","        x1 = self.Conv1(x)\n","\n","        x2 = self.Maxpool(x1)\n","        x2 = self.Conv2(x2)\n","        \n","        x3 = self.Maxpool(x2)\n","        x3 = self.Conv3(x3)\n","\n","        x4 = self.Maxpool(x3)\n","        x4 = self.Conv4(x4)\n","\n","        x5 = self.Maxpool(x4)\n","        x5 = self.Conv5(x5)\n","\n","        # decoding + concat path\n","        d5 = self.Up5(x5)\n","        d5 = torch.cat((x4,d5),dim=1)\n","        \n","        d5 = self.Up_conv5(d5)\n","        \n","        d4 = self.Up4(d5)\n","        d4 = torch.cat((x3,d4),dim=1)\n","        d4 = self.Up_conv4(d4)\n","\n","        d3 = self.Up3(d4)\n","        d3 = torch.cat((x2,d3),dim=1)\n","        d3 = self.Up_conv3(d3)\n","\n","        d2 = self.Up2(d3)\n","        d2 = torch.cat((x1,d2),dim=1)\n","        d2 = self.Up_conv2(d2)\n","\n","        d1 = self.Conv_1x1(d2)\n","\n","        return d1\n","\n","class Attention_block(nn.Module):\n","    def __init__(self,F_g,F_l,F_int):\n","        super(Attention_block,self).__init__()\n","        self.W_g = nn.Sequential(\n","            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n","            nn.BatchNorm2d(F_int)\n","            )\n","        \n","        self.W_x = nn.Sequential(\n","            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n","            nn.BatchNorm2d(F_int)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n","            nn.BatchNorm2d(1),\n","            nn.Sigmoid()\n","        )\n","        \n","        self.relu = nn.ReLU(inplace=True)\n","        \n","    def forward(self,g,x):\n","        g1 = self.W_g(g)\n","        x1 = self.W_x(x)\n","        psi = self.relu(g1+x1)\n","        psi = self.psi(psi)\n","\n","        return x*psi\n","\n","class AttU_Net(nn.Module):\n","    def __init__(self,img_ch=1,output_ch=4, drop_prob=0):\n","        super(AttU_Net,self).__init__()\n","        \n","        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n","\n","        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)\n","        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n","        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n","        self.Conv4 = conv_block(ch_in=256,ch_out=512, drop_block=True, block_size = 5, drop_prob = drop_prob)\n","        self.Conv5 = conv_block(ch_in=512,ch_out=1024, drop_block=True, block_size = 5, drop_prob = drop_prob)\n","\n","        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n","        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)\n","        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n","\n","        self.Up4 = up_conv(ch_in=512,ch_out=256)\n","        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)\n","        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n","        \n","        self.Up3 = up_conv(ch_in=256,ch_out=128)\n","        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)\n","        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n","        \n","        self.Up2 = up_conv(ch_in=128,ch_out=64)\n","        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n","        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n","\n","        self.Conv_1x1 = nn.Sequential(\n","            nn.Conv2d(64, output_ch, kernel_size=1,stride=1,padding=0), \n","            nn.Softmax(dim=1)\n","            )\n","\n","    def forward(self,x):\n","        # encoding path\n","        x1 = self.Conv1(x)\n","\n","        x2 = self.Maxpool(x1)\n","        x2 = self.Conv2(x2)\n","        \n","        x3 = self.Maxpool(x2)\n","        x3 = self.Conv3(x3)\n","\n","        x4 = self.Maxpool(x3)\n","        x4 = self.Conv4(x4)\n","\n","        x5 = self.Maxpool(x4)\n","        x5 = self.Conv5(x5)\n","\n","        # decoding + concat path\n","        d5 = self.Up5(x5)\n","        x4 = self.Att5(d5,x4)\n","        d5 = torch.cat((x4,d5),dim=1)        \n","        d5 = self.Up_conv5(d5)\n","        \n","        d4 = self.Up4(d5)\n","        x3 = self.Att4(d4,x3)\n","        d4 = torch.cat((x3,d4),dim=1)\n","        d4 = self.Up_conv4(d4)\n","\n","        d3 = self.Up3(d4)\n","        x2 = self.Att3(d3,x2)\n","        d3 = torch.cat((x2,d3),dim=1)\n","        d3 = self.Up_conv3(d3)\n","\n","        d2 = self.Up2(d3)\n","        x1 = self.Att2(d2,x1)\n","        d2 = torch.cat((x1,d2),dim=1)\n","        d2 = self.Up_conv2(d2)\n","\n","        d1 = self.Conv_1x1(d2)\n","\n","        return d1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmiuMlLPpkHS"},"source":["# summary(AttU_Net(), (1,128,128))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2w2N89XDWTfg"},"source":[""],"execution_count":null,"outputs":[]}]}