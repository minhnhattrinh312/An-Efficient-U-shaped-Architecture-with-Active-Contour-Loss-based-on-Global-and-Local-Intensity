{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"5VxGrJjz9azU"},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","from torchsummary import summary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EVXRyU12y8NE"},"source":["##model"]},{"cell_type":"code","metadata":{"id":"meu_SynQIK4s"},"source":["# https://github.com/miguelvr/dropblock/blob/master/dropblock/dropblock.py\n","class DropBlock2D(nn.Module):\n","    def __init__(self, drop_prob, block_size):\n","        super(DropBlock2D, self).__init__()\n","        self.drop_prob = drop_prob\n","        self.block_size = block_size\n","    def forward(self, x):\n","        # shape: (bsize, channels, height, width)\n","        assert x.dim() == 4, \\\n","            \"Expected input with 4 dimensions (bsize, channels, height, width)\"\n","        if not self.training or self.drop_prob == 0.:\n","            return x\n","        else:\n","            # get gamma value\n","            gamma = self.drop_prob / (self.block_size ** 2)\n","            # sample mask\n","            mask = (torch.rand(x.shape[0], *x.shape[2:], device= x.device) < gamma).float()\n","            # compute block mask\n","            block_mask = self._compute_block_mask(mask)\n","            # apply block mask\n","            out = x * block_mask[:, None, :, :]\n","            # scale output\n","            out = out * block_mask.numel() / block_mask.sum()\n","            return out\n","    def _compute_block_mask(self, mask):\n","        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n","                                  kernel_size=(self.block_size, self.block_size),\n","                                  stride=(1, 1),\n","                                  padding=self.block_size // 2)\n","\n","        if self.block_size % 2 == 0:\n","            block_mask = block_mask[:, :, :-1, :-1]\n","        block_mask = 1 - block_mask.squeeze(1)\n","        return block_mask\n","\n","class DropBlock3D(DropBlock2D):\n","    def __init__(self, drop_prob, block_size):\n","        super(DropBlock3D, self).__init__(drop_prob, block_size)\n","    def forward(self, x):\n","        # shape: (bsize, channels, depth, height, width)\n","        assert x.dim() == 5, \\\n","            \"Expected input with 5 dimensions (bsize, channels, depth, height, width)\"\n","        if not self.training or self.drop_prob == 0.:\n","            return x\n","        else:\n","            # get gamma value\n","            gamma = self.drop_prob / (self.block_size ** 3)\n","            # sample mask\n","            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n","            # place mask on input device\n","            mask = mask.to(x.device)\n","            # compute block mask\n","            block_mask = self._compute_block_mask(mask)\n","            # apply block mask\n","            out = x * block_mask[:, None, :, :, :]\n","            # scale output\n","            out = out * block_mask.numel() / block_mask.sum()\n","            return out\n","    def _compute_block_mask(self, mask):\n","        block_mask = F.max_pool3d(input=mask[:, None, :, :, :],\n","                                  kernel_size=(self.block_size, self.block_size, self.block_size),\n","                                  stride=(1, 1, 1),\n","                                  padding=self.block_size // 2)\n","        if self.block_size % 2 == 0:\n","            block_mask = block_mask[:, :, :-1, :-1, :-1]\n","        block_mask = 1 - block_mask.squeeze(1)\n","        return block_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YtzETDt9LZx"},"source":["class CBAM(nn.Module):\n","    def __init__(self, in_channel, reduction_ratio = 8):\n","        super().__init__()\n","        self.hid_channel = max(1, in_channel // reduction_ratio)\n","        self.globalAvgPool = nn.AdaptiveAvgPool2d(1)\n","        self.globalMaxPool = nn.AdaptiveMaxPool2d(1)\n","        # Shared MLP.\n","        self.fc = nn.Sequential(nn.Conv2d(in_channel, self.hid_channel, 1, bias=False),\n","                               nn.Mish(),\n","                               nn.Conv2d(self.hid_channel, in_channel, 1, bias=False))\n","        self.sigmoid = nn.Sigmoid()\n","        self.conv1 = nn.Conv2d(2, 1, kernel_size=7,\n","                               stride=1, padding=3, bias=False)\n","    def forward(self, x):\n","        ''' Channel attention '''\n","        avgOut = self.fc(self.globalAvgPool(x))\n","        maxOut = self.fc(self.globalMaxPool(x))\n","        Mc = self.sigmoid(avgOut + maxOut)\n","        Mf1 = Mc * x\n","\n","        ''' Spatial attention. '''\n","        avg_out = torch.mean(Mf1, dim=1, keepdim=True)\n","        max_out, _ = torch.max(Mf1, dim=1, keepdim=True)\n","\n","        Ms = torch.cat([max_out, avg_out], dim=1)\n","        Ms = self.sigmoid(self.conv1(Ms))\n","        Mf2 = Ms * Mf1\n","        return Mf2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaJr5QRJiv0C"},"source":["class ConvBn(nn.Sequential):\n","    def __init__(self, in_channel, out_channel, kernel_size = 3,\n","                 padding = 1, drop_block=False, block_size = 1, drop_prob = 0):\n","        super().__init__()\n","        self.add_module(\"conv\",nn.Conv2d(in_channel, out_channel, kernel_size, padding = padding,bias=False))\n","        if drop_block:\n","            self.add_module(\"drop_block\", DropBlock2D(block_size = block_size, drop_prob = drop_prob))\n","        self.add_module(\"bn\", nn.BatchNorm2d(out_channel))\n","        self.add_module(\"mish\", nn.Mish())\n","        self.add_module(\"cbam\", CBAM(out_channel))\n","\n","class DownSampleBlock(nn.Sequential):\n","    def __init__(self, in_channel, block_size = 1, drop_prob = 0):\n","        super().__init__()\n","        out_channel = in_channel // 2\n","        self.add_module(\"conv1\", nn.Conv2d(in_channel, out_channel, 1, bias=False))\n","        self.add_module(\"drop_block1\", DropBlock2D(block_size = block_size, drop_prob = drop_prob))\n","        self.add_module(\"bn\", nn.BatchNorm2d(out_channel))\n","        self.add_module(\"mish\", nn.Mish())\n","        self.add_module(\"cbam\", CBAM(out_channel))\n","        self.add_module(\"conv2\", nn.Conv2d(out_channel, out_channel, 2, 2, bias=False))\n","        self.add_module(\"drop_block2\", DropBlock2D(block_size = block_size, drop_prob = drop_prob))\n","\n","\n","class AttentionBlock(nn.Module):\n","    def __init__(self, in_channel, in_channel_skip, out_channel):\n","        super().__init__()\n","        self.conv_input = nn.Sequential(\n","            nn.Conv2d(in_channel, out_channel, 1, padding = 0, bias=False),\n","            nn.BatchNorm2d(out_channel),\n","            nn.ConvTranspose2d(out_channel, out_channel, 2, 2),\n","            CBAM(out_channel)\n","        )\n","        self.conv_skip = nn.Sequential(\n","            nn.Conv2d(in_channel_skip, out_channel, 1, bias = False),\n","            nn.BatchNorm2d(out_channel),\n","        )\n","        self.mixed_weight = nn.Sequential(\n","            nn.Mish(),\n","            nn.Conv2d(out_channel, 1, 1, bias = False),\n","            nn.BatchNorm2d(1),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, x, skip):\n","        input_weight = self.conv_input(x)\n","        skip_weight = self.conv_skip(skip)\n","        output_weight = self.mixed_weight(input_weight + skip_weight)\n","        return output_weight * skip\n","\n","class DenseLayer(nn.Module):\n","    def __init__(self, in_channel, grow_rate):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            ConvBn(in_channel, grow_rate*4,kernel_size=1, padding=0),\n","            ConvBn(grow_rate*4, grow_rate)\n","        )\n","    def forward(self, x):\n","        output = self.layer(x)\n","        return torch.cat([output, x], dim = 1)\n","\n","class DenseBlock(nn.Sequential):\n","    def __init__(self, in_channel, grow_rate, repetition):\n","        super().__init__()\n","        for i in range(repetition):\n","            layer = DenseLayer(in_channel+i*grow_rate, grow_rate)\n","            self.add_module(f\"dense_layer_{i+1}\", layer)\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, in_channel, in_channel_skip, out_channel,\n","                 block_size = 1, drop_prob = 0):\n","        super().__init__()\n","        self.conv_trans = nn.ConvTranspose2d(in_channel, out_channel, 2, 2)\n","        self.attention = AttentionBlock(in_channel, in_channel_skip, out_channel)\n","        self.convbn = ConvBn(in_channel_skip + out_channel, out_channel, drop_block=True,\n","                            block_size = block_size, drop_prob = drop_prob)\n","\n","    def forward(self, x, skip):\n","        output = self.conv_trans(x)\n","        attention = self.attention(x, skip)\n","        output = torch.cat([output, attention], dim=1)\n","        return self.convbn(output)\n","\n","class UpsampleBlock(nn.Sequential):\n","    def __init__(self,  in_channel, out_channel, times):\n","        super().__init__()\n","        for i in range(times):\n","            channel = in_channel if i == 0 else out_channel\n","            self.add_module(f\"convtrans{i+1}\", nn.ConvTranspose2d(channel, out_channel, 2, 2))\n","            self.add_module(f\"cbam{i+1}\", CBAM(out_channel))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rI9bdb4CCHxO"},"source":["class SegNet(nn.Module):\n","    def __init__(self, input_channel = 1, in_channel = 32,\n","                 num_classes = 4, drop_prob = 0):\n","        super().__init__()\n","        self.conv1 = nn.Sequential(\n","            ConvBn(input_channel, in_channel),\n","            ConvBn(in_channel, in_channel)\n","        )\n","        self.block = 4\n","        grow_list = [16, 32, 64, 64, 64]\n","        repetition_list = [6, 6, 6, 6, 6]\n","        block_list = [5, 4, 3, 2]\n","        ch_decoder = [256, 128, 64, 32]\n","        in_ch_skip = []\n","        self.dense_list = nn.ModuleList()\n","        self.downsample_list = nn.ModuleList()\n","        self.decoder_list = nn.ModuleList()\n","        self.up_sample_list = nn.ModuleList()\n","\n","        for i in range(self.block):\n","            self.dense_list.append(DenseBlock(in_channel, grow_list[i], repetition_list[i]))\n","            in_channel += repetition_list[i] * grow_list[i]\n","            in_ch_skip.append(in_channel)\n","            self.downsample_list.append(DownSampleBlock(in_channel, block_list[i], drop_prob))\n","            in_channel = in_channel // 2\n","\n","        i+=1\n","        self.bottle_neck = DenseBlock(in_channel, grow_list[i], repetition_list[i])\n","        in_channel += repetition_list[i] * grow_list[i]\n","        for i in range(self.block):\n","            self.decoder_list.append(DecoderBlock(in_channel, in_ch_skip[-i-1], ch_decoder[i],\n","                                                  block_list[-i-1], drop_prob))\n","            self.up_sample_list.append(UpsampleBlock(in_channel, num_classes, self.block-i))\n","            in_channel = ch_decoder[i]\n","        in_channel += self.block * num_classes\n","\n","        self.conv2 = nn.Sequential(\n","            nn.BatchNorm2d(in_channel),\n","            nn.Mish(),\n","            nn.Conv2d(in_channel, num_classes, kernel_size=1, padding=0),\n","            nn.Softmax(dim=1)\n","            )\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        encoder_for_cat = []\n","        output_cat = []\n","        for i in range(self.block):\n","            x = self.dense_list[i](x)\n","            encoder_for_cat.append(x)\n","            x = self.downsample_list[i](x)\n","        x = self.bottle_neck(x)\n","        output_cat.append(self.up_sample_list[0](x))\n","        for i in range(self.block):\n","            x = self.decoder_list[i](x, encoder_for_cat[-i-1])\n","            if i < self.block-1:\n","                output_cat.append(self.up_sample_list[i+1](x))\n","        output_cat.append(x)\n","        output = torch.cat(output_cat, dim=1)\n","        output = self.conv2(output)\n","\n","        return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nacrcoafPxx"},"source":["# summary(SegNet(), (1,128,128))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"im0rUSIhA4mw"},"source":["# S = SegNet(drop_prob=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muZcfplm_y1a"},"source":["# for layer in S.modules():\n","#     if isinstance(layer, DropBlock2D):\n","#         layer.drop_prob = 0.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cwVKBoZvxfnf"},"source":["# for layer in S.modules():\n","#     if isinstance(layer, DropBlock2D):\n","#         print(layer.drop_prob)"],"execution_count":null,"outputs":[]}]}