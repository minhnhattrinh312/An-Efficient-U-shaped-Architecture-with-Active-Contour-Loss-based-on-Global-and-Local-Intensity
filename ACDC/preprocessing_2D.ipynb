{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"id":"eKLcUUwjEAog","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669041321093,"user_tz":-420,"elapsed":18586,"user":{"displayName":"Nháº­t Minh","userId":"18273474663690038880"}},"outputId":"1f8d9334-6a87-45cc-8789-c4cd721726ab"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"C21r7gQiGOFF"},"source":["from IPython.display import clear_output\n","import nibabel as nib\n","import glob\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","from PIL import Image\n","from tqdm import tqdm\n","from fastprogress import master_bar, progress_bar\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader,Dataset\n","import matplotlib.pyplot as plt\n","# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","clear_output()\n","NUM_CLASS = 4\n","\n","def center_crop(img, crop_size = (160, 160)):\n","    img_crop = np.zeros(crop_size)\n","    w_in, h_in, d_in = img.shape\n","    img_crop = np.zeros((*crop_size,d_in))\n","    w_out, h_out = crop_size\n","    sub_w = max((w_in - w_out)//2-20, 0)\n","    sub_h = max((h_in - h_out)//2-10, 0)\n","\n","    img_clone = img[sub_w:sub_w+w_out, sub_h:sub_h+h_out]\n","    img_crop[:img_clone.shape[0], :img_clone.shape[1]] = img_clone\n","    return img_crop\n","\n","def crop2img(crop_image, initial_image):\n","    output = np.zeros_like(initial_image)\n","    w_in, h_in, _ = initial_image.shape\n","    w_out, h_out, _ = crop_image.shape\n","    sub_w = max((w_in - w_out)//2-20, 0)\n","    sub_h = max((h_in - h_out)//2-10, 0)\n","    output[sub_w:sub_w+w_out, sub_h:sub_h+h_out] = crop_image\n","    return output\n","\n","def min_max_preprocess(image, low_perc=1, high_perc=99):\n","    \"\"\"Main pre-processing function used for the challenge (seems to work the best).\n","    Remove outliers voxels first, then min-max scale.\n","    \"\"\"\n","    image = np.array(image)\n","    non_zeros = image > 0\n","    low, high = np.percentile(image[non_zeros], [low_perc, high_perc])\n","    image = np.clip(image, low, high)\n","    image = (image - low) / (high - low)\n","    return image.astype(np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5OEE2RJ1KB8"},"source":["# %cd /content/drive/MyDrive/dataACDCA\n","# all_files = sorted(glob.glob(\"./training/*\"))\n","# np.random.seed(42)\n","# np.random.shuffle(all_files)\n","# train_count = 70\n","# train_files = all_files[:train_count]\n","# test_files = all_files[train_count : train_count + 10]+all_files[-10:]\n","# valid_files = all_files[train_count + 10: train_count + 20]\n","\n","#def get_data(): #list_file\n","#     x_list = []\n","#     y_list = []\n","#     for files in tqdm(list_file):\n","#         list_image = [x for x in glob.glob(files+\"/*\") if x.find('frame') != -1 and x.find('gt') == -1]\n","#         for image_name in list_image:\n","#             num = image_name.find(\"nii\")\n","#             mask_name = image_name[:num-1] +\"_gt.nii.gz\"\n","#             image = nib.load(image_name).get_fdata().astype(np.uint16)\n","#             label = nib.load(mask_name).get_fdata().astype(np.uint8)\n","#             image = center_crop(image)\n","#             label = center_crop(label)\n","#             for z in range(image.shape[-1]):\n","#                 sub_image = image[...,z]\n","#                 sub_label = label[...,z]\n","#                 y_list.append(sub_label)\n","#                 x_list.append(sub_image)\n","#    images = np.load('/content/drive/MyDrive/MRI_ACDC/Dataset2/Sunny_Brook_MRI/x_test_crop_128_endo_sun09.npy') #np.asarray(x_list)\n","#    masks = np.load('/content/drive/MyDrive/MRI_ACDC/Dataset2/Sunny_Brook_MRI/y_test_crop_128_endo_sun09.npy') #np.asarray(y_list)\n","#    return images, masks\n","#x_train, y_train =  get_data()\n","# x_val, y_val =  get_data(valid_files)\n","# x_test, y_test =  get_data(test_files)\n","\n","#np.savez_compressed('/content/drive/MyDrive/MRI_ACDC/Dataset2/Sunny_Brook_MRI/test_crop_128_endo_sun09', image=x_train, mask=y_train)\n","# np.savez_compressed('ACDC_val160', image=x_val, mask=y_val)\n","# np.savez_compressed('ACDC_test160', image=x_test, mask=y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xrat9cA8wPm5"},"source":["# data = np.load(\"/content/drive/MyDrive/dataACDCA/ACDC_train160.npz\")\n","# x_train, y_train = data[\"image\"], data[\"mask\"]\n","# %cd /content/drive/MyDrive/dataACDCA\n","# x_list, y_list = [], []\n","# for i in tqdm(range(x_train.shape[0])):\n","#     image, mask = Image.fromarray(x_train[i]),  Image.fromarray(y_train[i])\n","#     x_list.append(image)\n","#     y_list.append(mask)\n","\n","#     sub_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n","#     sub_label = mask.transpose(Image.FLIP_TOP_BOTTOM)\n","#     y_list.append(sub_label)\n","#     x_list.append(sub_image)\n","\n","#     sub_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n","#     sub_label = mask.transpose(Image.FLIP_LEFT_RIGHT)\n","#     y_list.append(sub_label)\n","#     x_list.append(sub_image)\n","\n","#     degree = np.random.uniform(-30,30)\n","#     sub_image = image.rotate(degree, Image.NEAREST)\n","#     sub_label = mask.rotate(degree, Image.NEAREST)\n","#     y_list.append(sub_label)\n","#     x_list.append(sub_image)\n","# images = np.stack(x_list)\n","# masks = np.stack(y_list)\n","# np.savez_compressed(\"ACDC_train_aug160\", image=images, mask=masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLqYno7Qfj_H"},"source":["class ACDCLoader(Dataset):\n","    def __init__(self, images, masks,\n","                 transform=True, typeData = \"train\"):\n","        self.transform = transform if typeData == \"train\" else False  # augment data bool\n","        self.typeData = typeData\n","        self.images = images\n","        self.masks = masks\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def rotate(self, image, mask, degrees=(-30,30), p=0.3):\n","        if torch.rand(1) < p:\n","            degree = np.random.uniform(*degrees)\n","            image = image.rotate(degree, Image.NEAREST)\n","            mask = mask.rotate(degree, Image.NEAREST)\n","        return image, mask\n","    def horizontal_flip(self, image, mask, p=0.5):\n","        if torch.rand(1) < p:\n","            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n","            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n","        return image, mask\n","    def vertical_flip(self, image, mask, p=0.5):\n","        if torch.rand(1) < p:\n","            image = image.transpose(Image.FLIP_TOP_BOTTOM)\n","            mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n","        return image, mask\n","\n","    def augment(self, image, mask):\n","        image, mask = self.rotate(image, mask)\n","        image, mask = self.horizontal_flip(image, mask)\n","        image, mask = self.vertical_flip(image, mask)\n","        return image, mask\n","\n","    def __getitem__(self, idx):\n","        image = Image.fromarray(self.images[idx])\n","        mask = Image.fromarray(self.masks[idx])\n","    ####################### augmentation data ##############################\n","        if self.transform:\n","            image, mask = self.augment(image, mask)\n","        image = min_max_preprocess(image)\n","        image = torch.from_numpy(image[np.newaxis])\n","\n","        mask = np.asarray(mask, np.int64)\n","        mask = torch.from_numpy(mask[np.newaxis])\n","        return image, mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDaXFr94eodZ"},"source":["##loss"]},{"cell_type":"code","metadata":{"id":"lpy56sQyeuvg"},"source":["class SemiActiveLoss(nn.Module):\n","    def __init__(self, device, alpha =1e-9, beta = 1e-1, lamda = 1e-3):\n","        super().__init__()\n","        self.device = device\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.lamda = lamda\n","    def LevelsetLoss(self, image, y_pred, kernel_size=5, smooth=1e-5):\n","        kernel = torch.ones(1, y_pred.size(1), kernel_size, kernel_size, device=self.device) / kernel_size**2\n","        padding = kernel_size //2\n","        lossRegion = 0.0\n","        y_pred_fuzzy = y_pred\n","        for ich in range(image.size(1)):\n","            target_ = image[:,ich:ich+1]\n","            pcentroid_local = F.conv2d(target_ * y_pred_fuzzy + smooth, kernel, padding = padding) \\\n","                                / F.conv2d(y_pred_fuzzy + smooth, kernel, padding = padding)\n","            plevel_local = target_ - pcentroid_local\n","            loss_local = plevel_local * plevel_local * y_pred_fuzzy\n","\n","            pcentroid_global = torch.sum(target_ * y_pred_fuzzy, dim=(2,3),keepdim=True) \\\n","                                / torch.sum(y_pred_fuzzy+smooth, dim=(2,3),keepdim = True)\n","            plevel_global = target_ - pcentroid_global\n","            loss_global = plevel_global * plevel_global * y_pred_fuzzy\n","\n","            lossRegion += torch.sum(loss_local) + self.beta * torch.sum(loss_global)\n","        return lossRegion\n","    def GradientLoss(self, y_pred, penalty = \"l1\"):\n","        dH = torch.abs(y_pred[...,1:] - y_pred[...,:-1])\n","        dW = torch.abs(y_pred[:,:,1:] - y_pred[:,:,:-1])\n","        if penalty == \"l2\":\n","            dH = dH * dH\n","            dW = dW * dW\n","        loss =  torch.sum(dH) +  torch.sum(dW)\n","        return loss\n","    def ActiveContourLoss(self, y_true, y_pred, smooth=1e-5):\n","        dim = (1,2,3)\n","        yTrueOnehot = torch.zeros(y_true.size(0), NUM_CLASS, y_true.size(2), y_true.size(3), device=self.device)\n","        yTrueOnehot = torch.scatter(yTrueOnehot, 1, y_true, 1)[:,1:]\n","        y_pred = y_pred[:,1:]\n","\n","        active = - torch.log(1-y_pred+smooth) * (1-yTrueOnehot) - torch.log(y_pred+smooth) * yTrueOnehot\n","        loss = torch.sum(active, dim = dim) / torch.sum(yTrueOnehot + y_pred - yTrueOnehot * y_pred +smooth, dim = dim)\n","        return torch.mean(loss)\n","\n","    def forward(self, image, y_true, y_pred):\n","        active = self.ActiveContourLoss(y_true, y_pred)\n","        levelset =  self.LevelsetLoss(image, y_pred)\n","        length = self.GradientLoss(y_pred)\n","        return active + self.alpha * (levelset + self.lamda * length)\n","\n","class CrossEntropy(nn.Module):\n","    def __init__(self, device):\n","        super().__init__()\n","        self.device = device\n","    def forward(self, y_true, y_pred):\n","        yTrueOnehot = torch.zeros(y_true.size(0), NUM_CLASS, y_true.size(2), y_true.size(3), device=self.device)\n","        yTrueOnehot = torch.scatter(yTrueOnehot, 1, y_true, 1)\n","\n","        loss = torch.sum(-yTrueOnehot * torch.log(y_pred+1e-10))\n","        return loss / (y_true.size(0) * y_true.size(2) * y_true.size(3))\n","class DiceLoss(nn.Module):\n","    def __init__(self, device):\n","        super().__init__()\n","        self.device = device\n","    def forward(self, y_true, y_pred):\n","        yTrueOnehot = torch.zeros(y_true.size(0), NUM_CLASS, y_true.size(2), y_true.size(3), device=self.device)\n","        yTrueOnehot = torch.scatter(yTrueOnehot, 1, y_true, 1)[:, 1:]\n","        y_pred = y_pred[:,1:]\n","\n","        intersection = torch.sum(yTrueOnehot * y_pred, dim=[1,2,3])\n","        cardinality  = torch.sum(yTrueOnehot + y_pred , dim=[1,2,3])\n","        loss = 1.0-torch.mean((2. * intersection + 1e-5) / (cardinality + 1e-5))\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnPx9nmRfEyp"},"source":["##metrics"]},{"cell_type":"code","metadata":{"id":"IGrw4VZPfGQ3"},"source":["def dice_rv(y_true, y_pred, smooth = 1e-4):\n","    y_pred = torch.argmax(y_pred, dim=1, keepdim = True)\n","    y_pred = torch.where(y_pred == 1, 1, 0)\n","    y_true = torch.where(y_true == 1, 1, 0)\n","    intersection = torch.sum(y_true * y_pred, dim=[1,2,3])\n","    cardinality  = torch.sum(y_true + y_pred , dim=[1,2,3])\n","    return torch.mean((2. * intersection + smooth) / (cardinality + smooth), dim=0)\n","\n","def dice_myo(y_true, y_pred, smooth = 1e-4):\n","    y_pred = torch.argmax(y_pred, dim=1, keepdim = True)\n","    y_pred = torch.where(y_pred == 2, 1, 0)\n","    y_true = torch.where(y_true == 2, 1, 0)\n","    intersection = torch.sum(y_true * y_pred, dim=[1,2,3])\n","    cardinality  = torch.sum(y_true + y_pred , dim=[1,2,3])\n","    return torch.mean((2. * intersection + smooth) / (cardinality + smooth), dim=0)\n","\n","def dice_lv(y_true, y_pred, smooth = 1e-4):\n","    y_pred = torch.argmax(y_pred, dim=1, keepdim = True)\n","    y_pred = torch.where(y_pred == 3, 1, 0)\n","    y_true = torch.where(y_true == 3, 1, 0)\n","    intersection = torch.sum(y_true * y_pred, dim=[1,2,3])\n","    cardinality  = torch.sum(y_true + y_pred , dim=[1,2,3])\n","    return torch.mean((2. * intersection + smooth) / (cardinality + smooth), dim=0)\n","\n","def jac_rv(y_true, y_pred, smooth = 1e-4):\n","    y_pred = torch.argmax(y_pred, dim=1, keepdim = True)\n","    y_pred = torch.where(y_pred == 1, 1, 0)\n","    y_true = torch.where(y_true == 1, 1, 0)\n","    intersection = torch.sum(y_true * y_pred, dim=[1,2,3])\n","    cardinality  = torch.sum(y_true + y_pred , dim=[1,2,3])\n","    return torch.mean((1. * intersection + smooth) / (cardinality - intersection + smooth), dim=0)\n","\n","def jac_myo(y_true, y_pred, smooth = 1e-4):\n","    y_pred = torch.argmax(y_pred, dim=1, keepdim = True)\n","    y_pred = torch.where(y_pred == 2, 1, 0)\n","    y_true = torch.where(y_true == 2, 1, 0)\n","    intersection = torch.sum(y_true * y_pred, dim=[1,2,3])\n","    cardinality  = torch.sum(y_true + y_pred , dim=[1,2,3])\n","    return torch.mean((1. * intersection + smooth) / (cardinality -intersection + smooth), dim=0)\n","\n","def jac_lv(y_true, y_pred, smooth = 1e-4):\n","    y_pred = torch.argmax(y_pred, dim=1, keepdim = True)\n","    y_pred = torch.where(y_pred == 3, 1, 0)\n","    y_true = torch.where(y_true == 3, 1, 0)\n","    intersection = torch.sum(y_true * y_pred, dim=[1,2,3])\n","    cardinality  = torch.sum(y_true + y_pred , dim=[1,2,3])\n","    return torch.mean((1. * intersection + smooth) / (cardinality - intersection + smooth), dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B65eOb4VOE4G"},"source":["##optimizer"]},{"cell_type":"code","metadata":{"id":"fYPf1-J-OGl8"},"source":["import math\n","import torch\n","from torch.optim.optimizer import Optimizer\n","\n","\n","class Nadam(Optimizer):\n","    \"\"\"Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).\n","    It has been proposed in `Incorporating Nesterov Momentum into Adam`__.\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 2e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        schedule_decay (float, optional): momentum schedule decay (default: 4e-3)\n","    __ http://cs229.stanford.edu/proj2015/054_report.pdf\n","    __ http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n","        Originally taken from: https://github.com/pytorch/pytorch/pull/1408\n","        NOTE: Has potential issues but does work well on some problems.\n","    \"\"\"\n","\n","    def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0, schedule_decay=4e-3):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        defaults = dict(\n","            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, schedule_decay=schedule_decay)\n","        super(Nadam, self).__init__(params, defaults)\n","\n","    @torch.no_grad()\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            with torch.enable_grad():\n","                loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['m_schedule'] = 1.\n","                    state['exp_avg'] = torch.zeros_like(p)\n","                    state['exp_avg_sq'] = torch.zeros_like(p)\n","\n","                # Warming momentum schedule\n","                m_schedule = state['m_schedule']\n","                schedule_decay = group['schedule_decay']\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","                eps = group['eps']\n","                state['step'] += 1\n","                t = state['step']\n","                bias_correction2 = 1 - beta2 ** t\n","\n","                if group['weight_decay'] != 0:\n","                    grad = grad.add(p, alpha=group['weight_decay'])\n","\n","                momentum_cache_t = beta1 * (1. - 0.5 * (0.96 ** (t * schedule_decay)))\n","                momentum_cache_t_1 = beta1 * (1. - 0.5 * (0.96 ** ((t + 1) * schedule_decay)))\n","                m_schedule_new = m_schedule * momentum_cache_t\n","                m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\n","                state['m_schedule'] = m_schedule_new\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(grad, alpha=1. - beta1)\n","                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1. - beta2)\n","\n","                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n","                p.addcdiv_(grad, denom, value=-group['lr'] * (1. - momentum_cache_t) / (1. - m_schedule_new))\n","                p.addcdiv_(exp_avg, denom, value=-group['lr'] * momentum_cache_t_1 / (1. - m_schedule_next))\n","\n","        return loss"],"execution_count":null,"outputs":[]}]}