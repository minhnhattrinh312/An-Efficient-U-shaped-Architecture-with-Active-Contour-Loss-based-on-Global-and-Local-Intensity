{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install einops==0.3.0\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from einops import rearrange, repeat\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, embedding_dim, head_num):\n","        super().__init__()\n","\n","        self.head_num = head_num\n","        self.dk = (embedding_dim // head_num) ** 1 / 2\n","\n","        self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n","        self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n","\n","    def forward(self, x, mask=None):\n","        qkv = self.qkv_layer(x)\n","\n","        query, key, value = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.head_num))\n","        energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n","\n","        if mask is not None:\n","            energy = energy.masked_fill(mask, -np.inf)\n","\n","        attention = torch.softmax(energy, dim=-1)\n","\n","        x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n","\n","        x = rearrange(x, \"b h t d -> b t (h d)\")\n","        x = self.out_attention(x)\n","\n","        return x\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, embedding_dim, mlp_dim):\n","        super().__init__()\n","\n","        self.mlp_layers = nn.Sequential(\n","            nn.Linear(embedding_dim, mlp_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(mlp_dim, embedding_dim),\n","            nn.Dropout(0.1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.mlp_layers(x)\n","\n","        return x\n","\n","\n","class TransformerEncoderBlock(nn.Module):\n","    def __init__(self, embedding_dim, head_num, mlp_dim):\n","        super().__init__()\n","\n","        self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n","        self.mlp = MLP(embedding_dim, mlp_dim)\n","\n","        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n","        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n","\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        _x = self.multi_head_attention(x)\n","        _x = self.dropout(_x)\n","        x = x + _x\n","        x = self.layer_norm1(x)\n","\n","        _x = self.mlp(x)\n","        x = x + _x\n","        x = self.layer_norm2(x)\n","\n","        return x\n","\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n","        super().__init__()\n","\n","        self.layer_blocks = nn.ModuleList(\n","            [TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)])\n","\n","    def forward(self, x):\n","        for layer_block in self.layer_blocks:\n","            x = layer_block(x)\n","\n","        return x\n","\n","\n","class ViT(nn.Module):\n","    def __init__(self, img_dim, in_channels, embedding_dim, head_num, mlp_dim,\n","                 block_num, patch_dim, classification=True, num_classes=1):\n","        super().__init__()\n","\n","        self.patch_dim = patch_dim\n","        self.classification = classification\n","        self.num_tokens = (img_dim // patch_dim) ** 2\n","        self.token_dim = in_channels * (patch_dim ** 2)\n","\n","        self.projection = nn.Linear(self.token_dim, embedding_dim)\n","        self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n","\n","        self.dropout = nn.Dropout(0.1)\n","\n","        self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n","\n","        if self.classification:\n","            self.mlp_head = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        img_patches = rearrange(x,\n","                                'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n","                                patch_x=self.patch_dim, patch_y=self.patch_dim)\n","\n","        batch_size, tokens, _ = img_patches.shape\n","\n","        project = self.projection(img_patches)\n","        token = repeat(self.cls_token, 'b ... -> (b batch_size) ...',\n","                       batch_size=batch_size)\n","\n","        patches = torch.cat([token, project], dim=1)\n","        patches += self.embedding[:tokens + 1, :]\n","\n","        x = self.dropout(patches)\n","        x = self.transformer(x)\n","        x = self.mlp_head(x[:, 0, :]) if self.classification else x[:, 1:, :]\n","\n","        return x\n","\n","class EncoderBottleneck(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n","        super().__init__()\n","\n","        self.downsample = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","        width = int(out_channels * (base_width / 64))\n","\n","        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, bias=False)\n","        self.norm1 = nn.BatchNorm2d(width)\n","\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n","        self.norm2 = nn.BatchNorm2d(width)\n","\n","        self.conv3 = nn.Conv2d(width, out_channels, kernel_size=1, stride=1, bias=False)\n","        self.norm3 = nn.BatchNorm2d(out_channels)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x_down = self.downsample(x)\n","\n","        x = self.conv1(x)\n","        x = self.norm1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.norm2(x)\n","        x = self.relu(x)\n","\n","        x = self.conv3(x)\n","        x = self.norm3(x)\n","        x = x + x_down\n","        x = self.relu(x)\n","\n","        return x\n","\n","\n","class DecoderBottleneck(nn.Module):\n","    def __init__(self, in_channels, out_channels, scale_factor=2):\n","        super().__init__()\n","\n","        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x, x_concat=None):\n","        x = self.upsample(x)\n","\n","        if x_concat is not None:\n","            x = torch.cat([x_concat, x], dim=1)\n","\n","        x = self.layer(x)\n","        return x\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.norm1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n","        self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n","        self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n","\n","        self.vit_img_dim = img_dim // patch_dim\n","        self.vit = ViT(self.vit_img_dim, out_channels * 8, out_channels * 8,\n","                       head_num, mlp_dim, block_num, patch_dim=1, classification=False)\n","\n","        self.conv2 = nn.Conv2d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n","        self.norm2 = nn.BatchNorm2d(512)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.norm1(x)\n","        x1 = self.relu(x)\n","\n","        x2 = self.encoder1(x1)\n","        x3 = self.encoder2(x2)\n","        x = self.encoder3(x3)\n","\n","        x = self.vit(x)\n","        x = rearrange(x, \"b (x y) c -> b c x y\", x=self.vit_img_dim, y=self.vit_img_dim)\n","\n","        x = self.conv2(x)\n","        x = self.norm2(x)\n","        x = self.relu(x)\n","\n","        return x, x1, x2, x3\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, out_channels, class_num):\n","        super().__init__()\n","\n","        self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 2)\n","        self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels)\n","        self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n","        self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n","\n","        self.conv1 = nn.Conv2d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n","\n","    def forward(self, x, x1, x2, x3):\n","        x = self.decoder1(x, x3)\n","        x = self.decoder2(x, x2)\n","        x = self.decoder3(x, x1)\n","        x = self.decoder4(x)\n","        x = self.conv1(x)\n","\n","        return x\n","\n","\n","class TransUNet(nn.Module):\n","    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n","        super().__init__()\n","\n","        self.encoder = Encoder(img_dim, in_channels, out_channels,\n","                               head_num, mlp_dim, block_num, patch_dim)\n","\n","        self.decoder = Decoder(out_channels, class_num)\n","\n","    def forward(self, x):\n","        x, x1, x2, x3 = self.encoder(x)\n","        x = self.decoder(x, x1, x2, x3)\n","\n","        return x\n"],"metadata":{"id":"KM5qrh5sIZIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    transunet = TransUNet(img_dim=128,\n","                          in_channels=3,\n","                          out_channels=128,\n","                          head_num=4,\n","                          mlp_dim=512,\n","                          block_num=8,\n","                          patch_dim=16,\n","                          class_num=1)\n","\n","    print(sum(p.numel() for p in transunet.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GafGsgxHIo-F","executionInfo":{"status":"ok","timestamp":1641317499051,"user_tz":-420,"elapsed":1044,"user":{"displayName":"Minh Nhat Trinh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUlruUlJibAIqBqbqLK2Tos6lKVv00A2KZfuwF=s64","userId":"12425805762404293245"}},"outputId":"472aef93-b997-4b3d-f58e-39a9fb1ffc46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["66882673\n"]}]},{"cell_type":"code","source":["!pip install timm\n","!pip install einops\n","import torch\n","import torch.nn as nn\n","import torch.utils.checkpoint as checkpoint\n","from einops import rearrange\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","def window_partition(x, window_size):\n","    \"\"\"\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): window size\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, H, W):\n","    \"\"\"\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): Window size\n","        H (int): Height of image\n","        W (int): Width of image\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    B = int(windows.shape[0] / (H * W / window_size / window_size))\n","    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n","    return x\n","\n","\n","class WindowAttention(nn.Module):\n","    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n","    It supports both of shifted and non-shifted window.\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal_(self.relative_position_bias_table, std=.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, N, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n","\n","    def flops(self, N):\n","        # calculate flops for 1 window with token length of N\n","        flops = 0\n","        # qkv = self.qkv(x)\n","        flops += N * self.dim * 3 * self.dim\n","        # attn = (q @ k.transpose(-2, -1))\n","        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n","        #  x = (attn @ v)\n","        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n","        # x = self.proj(x)\n","        flops += N * self.dim * self.dim\n","        return flops\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    r\"\"\" Swin Transformer Block.\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resulotion.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Window size.\n","        shift_size (int): Shift size for SW-MSA.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n","        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n","                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        if min(self.input_resolution) <= self.window_size:\n","            # if window size is larger than input resolution, we don't partition windows\n","            self.shift_size = 0\n","            self.window_size = min(self.input_resolution)\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n","            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if self.shift_size > 0:\n","            # calculate attention mask for SW-MSA\n","            H, W = self.input_resolution\n","            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n","            h_slices = (slice(0, -self.window_size),\n","                        slice(-self.window_size, -self.shift_size),\n","                        slice(-self.shift_size, None))\n","            w_slices = (slice(0, -self.window_size),\n","                        slice(-self.window_size, -self.shift_size),\n","                        slice(-self.shift_size, None))\n","            cnt = 0\n","            for h in h_slices:\n","                for w in w_slices:\n","                    img_mask[:, h, w, :] = cnt\n","                    cnt += 1\n","\n","            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n","            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n","            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","        else:\n","            attn_mask = None\n","\n","        self.register_buffer(\"attn_mask\", attn_mask)\n","\n","    def forward(self, x):\n","        H, W = self.input_resolution\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # partition windows\n","        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n","\n","        # W-MSA/SW-MSA\n","        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n","\n","        # merge windows\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n","\n","        # reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","        x = x.view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n","               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.input_resolution\n","        # norm1\n","        flops += self.dim * H * W\n","        # W-MSA/SW-MSA\n","        nW = H * W / self.window_size / self.window_size\n","        flops += nW * self.attn.flops(self.window_size * self.window_size)\n","        # mlp\n","        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n","        # norm2\n","        flops += self.dim * H * W\n","        return flops\n","\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","    Args:\n","        input_resolution (tuple[int]): Resolution of input feature.\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n","\n","        x = x.view(B, H, W, C)\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n","        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n","\n","    def flops(self):\n","        H, W = self.input_resolution\n","        flops = H * W * self.dim\n","        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n","        return flops\n","\n","class PatchExpand(nn.Module):\n","    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.expand = nn.Linear(dim, 2*dim, bias=False) if dim_scale==2 else nn.Identity()\n","        self.norm = norm_layer(dim // dim_scale)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        x = self.expand(x)\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        x = x.view(B, H, W, C)\n","        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C//4)\n","        x = x.view(B,-1,C//4)\n","        x= self.norm(x)\n","\n","        return x\n","\n","class FinalPatchExpand_X4(nn.Module):\n","    def __init__(self, input_resolution, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.dim_scale = dim_scale\n","        self.expand = nn.Linear(dim, 16*dim, bias=False)\n","        self.output_dim = dim \n","        self.norm = norm_layer(self.output_dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        x = self.expand(x)\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        x = x.view(B, H, W, C)\n","        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//(self.dim_scale**2))\n","        x = x.view(B,-1,self.output_dim)\n","        x= self.norm(x)\n","\n","        return x\n","\n","class BasicLayer(nn.Module):\n","    \"\"\" A basic Swin Transformer layer for one stage.\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resolution.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","\n","        # build blocks\n","        self.blocks = nn.ModuleList([\n","            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n","                                 num_heads=num_heads, window_size=window_size,\n","                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                                 mlp_ratio=mlp_ratio,\n","                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                 drop=drop, attn_drop=attn_drop,\n","                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                                 norm_layer=norm_layer)\n","            for i in range(depth)])\n","\n","        # patch merging layer\n","        if downsample is not None:\n","            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n","        else:\n","            self.downsample = None\n","\n","    def forward(self, x):\n","        for blk in self.blocks:\n","            if self.use_checkpoint:\n","                x = checkpoint.checkpoint(blk, x)\n","            else:\n","                x = blk(x)\n","        if self.downsample is not None:\n","            x = self.downsample(x)\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n","\n","    def flops(self):\n","        flops = 0\n","        for blk in self.blocks:\n","            flops += blk.flops()\n","        if self.downsample is not None:\n","            flops += self.downsample.flops()\n","        return flops\n","\n","class BasicLayer_up(nn.Module):\n","    \"\"\" A basic Swin Transformer layer for one stage.\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resolution.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, upsample=None, use_checkpoint=False):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","\n","        # build blocks\n","        self.blocks = nn.ModuleList([\n","            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n","                                 num_heads=num_heads, window_size=window_size,\n","                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                                 mlp_ratio=mlp_ratio,\n","                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                 drop=drop, attn_drop=attn_drop,\n","                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                                 norm_layer=norm_layer)\n","            for i in range(depth)])\n","\n","        # patch merging layer\n","        if upsample is not None:\n","            self.upsample = PatchExpand(input_resolution, dim=dim, dim_scale=2, norm_layer=norm_layer)\n","        else:\n","            self.upsample = None\n","\n","    def forward(self, x):\n","        for blk in self.blocks:\n","            if self.use_checkpoint:\n","                x = checkpoint.checkpoint(blk, x)\n","            else:\n","                x = blk(x)\n","        if self.upsample is not None:\n","            x = self.upsample(x)\n","        return x\n","\n","class PatchEmbed(nn.Module):\n","    r\"\"\" Image to Patch Embedding\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        if norm_layer is not None:\n","            self.norm = norm_layer(embed_dim)\n","        else:\n","            self.norm = None\n","\n","    def forward(self, x):\n","        B, C, H, W = x.shape\n","        # FIXME look at relaxing size constraints\n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        return x\n","\n","    def flops(self):\n","        Ho, Wo = self.patches_resolution\n","        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n","        if self.norm is not None:\n","            flops += Ho * Wo * self.embed_dim\n","        return flops\n","\n","\n","class SwinTransformerSys(nn.Module):\n","    r\"\"\" Swin Transformer\n","        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n","          https://arxiv.org/pdf/2103.14030\n","    Args:\n","        img_size (int | tuple(int)): Input image size. Default 224\n","        patch_size (int | tuple(int)): Patch size. Default: 4\n","        in_chans (int): Number of input image channels. Default: 3\n","        num_classes (int): Number of classes for classification head. Default: 1000\n","        embed_dim (int): Patch embedding dimension. Default: 96\n","        depths (tuple(int)): Depth of each Swin Transformer layer.\n","        num_heads (tuple(int)): Number of attention heads in different layers.\n","        window_size (int): Window size. Default: 7\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n","        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n","        drop_rate (float): Dropout rate. Default: 0\n","        attn_drop_rate (float): Attention dropout rate. Default: 0\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n","        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n","        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n","        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n","                 embed_dim=96, depths=[2, 2, 6, 2], depths_decoder=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n","                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n","                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n","                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n","                 use_checkpoint=False, final_upsample=\"expand_first\", **kwargs):\n","        super().__init__()\n","\n","        print(\"SwinTransformerSys expand initial----depths:{};depths_decoder:{};drop_path_rate:{};num_classes:{}\".format(depths,\n","        depths_decoder,drop_path_rate,num_classes))\n","\n","        self.num_classes = num_classes\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.ape = ape\n","        self.patch_norm = patch_norm\n","        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n","        self.num_features_up = int(embed_dim * 2)\n","        self.mlp_ratio = mlp_ratio\n","        self.final_upsample = final_upsample\n","\n","        # split image into non-overlapping patches\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","        num_patches = self.patch_embed.num_patches\n","        patches_resolution = self.patch_embed.patches_resolution\n","        self.patches_resolution = patches_resolution\n","\n","        # absolute position embedding\n","        if self.ape:\n","            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","            trunc_normal_(self.absolute_pos_embed, std=.02)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # stochastic depth\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","\n","        # build encoder and bottleneck layers\n","        self.layers = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n","                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n","                                                 patches_resolution[1] // (2 ** i_layer)),\n","                               depth=depths[i_layer],\n","                               num_heads=num_heads[i_layer],\n","                               window_size=window_size,\n","                               mlp_ratio=self.mlp_ratio,\n","                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                               drop=drop_rate, attn_drop=attn_drop_rate,\n","                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n","                               norm_layer=norm_layer,\n","                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n","                               use_checkpoint=use_checkpoint)\n","            self.layers.append(layer)\n","        \n","        # build decoder layers\n","        self.layers_up = nn.ModuleList()\n","        self.concat_back_dim = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            concat_linear = nn.Linear(2*int(embed_dim*2**(self.num_layers-1-i_layer)),\n","            int(embed_dim*2**(self.num_layers-1-i_layer))) if i_layer > 0 else nn.Identity()\n","            if i_layer ==0 :\n","                layer_up = PatchExpand(input_resolution=(patches_resolution[0] // (2 ** (self.num_layers-1-i_layer)),\n","                patches_resolution[1] // (2 ** (self.num_layers-1-i_layer))), dim=int(embed_dim * 2 ** (self.num_layers-1-i_layer)), dim_scale=2, norm_layer=norm_layer)\n","            else:\n","                layer_up = BasicLayer_up(dim=int(embed_dim * 2 ** (self.num_layers-1-i_layer)),\n","                                input_resolution=(patches_resolution[0] // (2 ** (self.num_layers-1-i_layer)),\n","                                                    patches_resolution[1] // (2 ** (self.num_layers-1-i_layer))),\n","                                depth=depths[(self.num_layers-1-i_layer)],\n","                                num_heads=num_heads[(self.num_layers-1-i_layer)],\n","                                window_size=window_size,\n","                                mlp_ratio=self.mlp_ratio,\n","                                qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                drop=drop_rate, attn_drop=attn_drop_rate,\n","                                drop_path=dpr[sum(depths[:(self.num_layers-1-i_layer)]):sum(depths[:(self.num_layers-1-i_layer) + 1])],\n","                                norm_layer=norm_layer,\n","                                upsample=PatchExpand if (i_layer < self.num_layers - 1) else None,\n","                                use_checkpoint=use_checkpoint)\n","            self.layers_up.append(layer_up)\n","            self.concat_back_dim.append(concat_linear)\n","\n","        self.norm = norm_layer(self.num_features)\n","        self.norm_up= norm_layer(self.embed_dim)\n","\n","        if self.final_upsample == \"expand_first\":\n","            print(\"---final upsample expand_first---\")\n","            self.up = FinalPatchExpand_X4(input_resolution=(img_size//patch_size,img_size//patch_size),dim_scale=4,dim=embed_dim)\n","            self.output = nn.Conv2d(in_channels=embed_dim,out_channels=self.num_classes,kernel_size=1,bias=False)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'absolute_pos_embed'}\n","\n","    @torch.jit.ignore\n","    def no_weight_decay_keywords(self):\n","        return {'relative_position_bias_table'}\n","\n","    #Encoder and Bottleneck\n","    def forward_features(self, x):\n","        x = self.patch_embed(x)\n","        if self.ape:\n","            x = x + self.absolute_pos_embed\n","        x = self.pos_drop(x)\n","        x_downsample = []\n","\n","        for layer in self.layers:\n","            x_downsample.append(x)\n","            x = layer(x)\n","\n","        x = self.norm(x)  # B L C\n","  \n","        return x, x_downsample\n","\n","    #Dencoder and Skip connection\n","    def forward_up_features(self, x, x_downsample):\n","        for inx, layer_up in enumerate(self.layers_up):\n","            if inx == 0:\n","                x = layer_up(x)\n","            else:\n","                x = torch.cat([x,x_downsample[3-inx]],-1)\n","                x = self.concat_back_dim[inx](x)\n","                x = layer_up(x)\n","\n","        x = self.norm_up(x)  # B L C\n","  \n","        return x\n","\n","    def up_x4(self, x):\n","        H, W = self.patches_resolution\n","        B, L, C = x.shape\n","        assert L == H*W, \"input features has wrong size\"\n","\n","        if self.final_upsample==\"expand_first\":\n","            x = self.up(x)\n","            x = x.view(B,4*H,4*W,-1)\n","            x = x.permute(0,3,1,2) #B,C,H,W\n","            x = self.output(x)\n","            \n","        return x\n","\n","    def forward(self, x):\n","        x, x_downsample = self.forward_features(x)\n","        x = self.forward_up_features(x,x_downsample)\n","        x = self.up_x4(x)\n","\n","        return x\n","\n","    def flops(self):\n","        flops = 0\n","        flops += self.patch_embed.flops()\n","        for i, layer in enumerate(self.layers):\n","            flops += layer.flops()\n","        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n","        flops += self.num_features * self.num_classes\n","        return flops\n","S = SwinTransformerSys()\n","print(sum(p.numel() for p in S.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blPiK94q4cFR","executionInfo":{"status":"ok","timestamp":1641465108527,"user_tz":-420,"elapsed":11852,"user":{"displayName":"Minh Nhat Trinh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUlruUlJibAIqBqbqLK2Tos6lKVv00A2KZfuwF=s64","userId":"12425805762404293245"}},"outputId":"9cf9ff11-d2b4-4679-bb52-559c2bc9d3c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.2)\n","SwinTransformerSys expand initial----depths:[2, 2, 6, 2];depths_decoder:[2, 2, 6, 2];drop_path_rate:0.1;num_classes:1000\n","---final upsample expand_first---\n","41475972\n"]}]},{"cell_type":"code","source":["import copy\n","import logging\n","import math\n","\n","class SwinUnet(nn.Module):\n","    def __init__(self, img_size=224, num_classes=21843, zero_head=False, vis=False):\n","        super(SwinUnet, self).__init__()\n","        self.num_classes = num_classes\n","        self.zero_head = zero_head\n","\n","\n","        self.swin_unet = SwinTransformerSys()\n","\n","    def forward(self, x):\n","        if x.size()[1] == 1:\n","            x = x.repeat(1,3,1,1)\n","        logits = self.swin_unet(x)\n","        return logits\n","\n","    def load_from(self, config):\n","        pretrained_path = config.MODEL.PRETRAIN_CKPT\n","        if pretrained_path is not None:\n","            print(\"pretrained_path:{}\".format(pretrained_path))\n","            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","            pretrained_dict = torch.load(pretrained_path, map_location=device)\n","            if \"model\"  not in pretrained_dict:\n","                print(\"---start load pretrained modle by splitting---\")\n","                pretrained_dict = {k[17:]:v for k,v in pretrained_dict.items()}\n","                for k in list(pretrained_dict.keys()):\n","                    if \"output\" in k:\n","                        print(\"delete key:{}\".format(k))\n","                        del pretrained_dict[k]\n","                msg = self.swin_unet.load_state_dict(pretrained_dict,strict=False)\n","                # print(msg)\n","                return\n","            pretrained_dict = pretrained_dict['model']\n","            print(\"---start load pretrained modle of swin encoder---\")\n","\n","            model_dict = self.swin_unet.state_dict()\n","            full_dict = copy.deepcopy(pretrained_dict)\n","            for k, v in pretrained_dict.items():\n","                if \"layers.\" in k:\n","                    current_layer_num = 3-int(k[7:8])\n","                    current_k = \"layers_up.\" + str(current_layer_num) + k[8:]\n","                    full_dict.update({current_k:v})\n","            for k in list(full_dict.keys()):\n","                if k in model_dict:\n","                    if full_dict[k].shape != model_dict[k].shape:\n","                        print(\"delete:{};shape pretrain:{};shape model:{}\".format(k,v.shape,model_dict[k].shape))\n","                        del full_dict[k]\n","\n","            msg = self.swin_unet.load_state_dict(full_dict, strict=False)\n","            # print(msg)\n","        else:\n","            print(\"none pretrain\")\n","S = SwinUnet()\n","print(sum(p.numel() for p in S.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rD1dIlFJ65Qv","executionInfo":{"status":"ok","timestamp":1641465109708,"user_tz":-420,"elapsed":1193,"user":{"displayName":"Minh Nhat Trinh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUlruUlJibAIqBqbqLK2Tos6lKVv00A2KZfuwF=s64","userId":"12425805762404293245"}},"outputId":"d8b71dbe-ef43-41bd-9692-9af7c2744217"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SwinTransformerSys expand initial----depths:[2, 2, 6, 2];depths_decoder:[2, 2, 6, 2];drop_path_rate:0.1;num_classes:1000\n","---final upsample expand_first---\n","41475972\n"]}]}]}